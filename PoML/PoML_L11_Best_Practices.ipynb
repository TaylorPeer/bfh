{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1654e7",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "source": [
    "# Jupyter Best Practices, Tips, & Tricks\n",
    "\n",
    "This notebook explains some basic best practicies when using Jupyter as well as a few final notes that may be useful for your course project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eefb8d6",
   "metadata": {},
   "source": [
    "## Markdown Cells\n",
    "\n",
    "Use markdown cells appropriately to make your notebook easily understandable for others (as well as for your future self). Structure headings hierarchically. Use only a single H1 tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b9f35",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Include all of your Python import statements in a single cell as the first code cell of your notebook. This helps organize all of your imports so you can be sure you are not importing the same dependency multiple times throughout your code. It also ensures that you can execute your notebook from start to finish without running into missing dependency errors.\n",
    "\n",
    "Some notebooks used earlier in the semester included imports throughout the notebook. This was for educational purposes only to introduce each import as it was used to make it clear which imports were necessary for each piece of functionality. For operational notebooks, however, this would be bad practice.\n",
    "\n",
    "Do not import dependencies that you ultimately do not use. If you initially use a dependency but then decide to remove it from the notebook later, remove the corresponding (now unused) import statement. Modern IDEs should be able to identify instances of this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from itertools import product\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris, fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9df0aab",
   "metadata": {},
   "source": [
    "## Manage Dependencies\n",
    "\n",
    "Use an external file (*requirements.txt*) to manage all dependencies **and their versions** used in your project.\n",
    "\n",
    "For example, a requirements.txt might look like this:\n",
    "\n",
    "scikit-learn==1.3.1<br/>\n",
    "pandas==2.1.4<br/>\n",
    "matplotlib==3.8.0<br/>\n",
    "\n",
    "Including the version number is important because the syntax used by individual libraries can change over time. If you program against the API of the current latest version and this changes in the future, someone using a future version may not be able to run your code if you did not specify which version of the dependency you were using at the time.\n",
    "\n",
    "You can find the corresponding version numbers for each dependency on [PyPi](https://pypi.org/), for example: https://pypi.org/project/scikit-learn/\n",
    "\n",
    "You can also print the version of each dependency you are using via *pip*, for example: *!pip show pandas*\n",
    "\n",
    "External tools like *pipreqs* (and its *pipreqsnb* extension for Jupyter notebooks) can also be used to generate a requirements.txt file. The *pip freeze* command is not recommended for this purpose since it includes all dependencies includes in your environment (and not just those that are actually used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e039f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pipreqsnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e9f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !cat requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5d2ca9",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "Some datasets may be missing certain features or measurements for individual rows. \n",
    "\n",
    "There are more advanced methods to handle these cases by *imputing* (essentially estimating by various means) these missing values, e.g. via functionality provided by Scikit-Learn (https://scikit-learn.org/stable/modules/impute.html) but generally it is also acceptable to simply ignore rows with missing values if they make up only a small subset of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde82c8",
   "metadata": {},
   "source": [
    "Sample DataFrame with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7071c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Letters': ['a', 'b', 'c', None, 'e'], 'Numbers': ['10', '20', '30', '40', '50']}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc67c39",
   "metadata": {},
   "source": [
    "Rows with missing values can be ignored or *dropped* via *dropna()*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c2bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f034f81",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "When tuning a model, you may want to experiment with various hyperparameter configurations. When tuning multiple hyperparameters, the number of their individual combinations can grow exponentially. This makes manual parameter tuning difficult.\n",
    "\n",
    "Scikit-Learn provides functionality to systematically run experiments across a range of configured hyperparameter configurations. This is known as performing a grid search.\n",
    "\n",
    "It is important to keep in mind that the configured search space can still result in extremely long processing times. Using some common sense and intuition to limit the search space to those parameter values that are likely to be useful is recommended.\n",
    "\n",
    "The grid search can also be run in parallel across threads using the *n_jobs* argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f8fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "# Note: parameter names must correspond with those available in the classifier\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Calculate the number of combinations\n",
    "num_combinations = 1\n",
    "for param_values in param_grid.values():\n",
    "    num_combinations *= len(param_values)\n",
    "    \n",
    "print(\"Number of parameter combinations to evaluate: {}\".format(num_combinations))\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "# n_jobs specifies the number of processes used for performing the grid search\n",
    "n_jobs = 1\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=n_jobs)\n",
    "\n",
    "# Perform grid search on the training data and record the start time\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Record the end time & time elapsed\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed grid search in {} seconds with {} jobs\".format(round(elapsed_time, 2), n_jobs))\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters: {}\".format(best_params))\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on Test Set: {}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae66e1f",
   "metadata": {},
   "source": [
    "## Ensemble Models\n",
    "\n",
    "Ensemble models can be used to combine the predictions of multiple underlying classifiers.\n",
    "\n",
    "One way to combine multiple classifiers is by allowing each classifier to \"vote\" and combining the votes.\n",
    "\n",
    "This approach allows combining multiple high-performing classifiers and may result in an ensemble classifier that is better than any of its individual parts.\n",
    "\n",
    "In practice, the resulting classifier is typically not significantly better than the best individual classifier and may not be worth the additional computational overhead and difficulty in interpreting results, depending on the use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define three different base classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "lr_classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Create a majority vote classifier (VotingClassifier)\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_classifier), \n",
    "        ('dt', dt_classifier), \n",
    "        ('lr', lr_classifier)],\n",
    "    voting='hard' # Use 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "# Train the majority vote classifier\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the majority vote classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the Majority Vote Classifier: {}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c55bd",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "Overfitting occurs when a model learns the training data \"too well\" by capturing noise or specific patterns that do not generalize well to new, unseen data. An overfit model performs very well on the training data but poorly on new, unseen data (e.g. the test set).\n",
    "\n",
    "Two common causes of overfitting are:\n",
    "\n",
    "- Model complexity: Overly complex models, especially those with a large number of parameters or features, are more prone to overfitting. Such models may fit the training data very closely but may not generalize well to new data.\n",
    "\n",
    "- Not enough training data: When the training dataset is small, the model may learn specific examples and noise in the data instead of capturing the underlying patterns. A small dataset may not represent the diversity of patterns present in the entire population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California housing dataset\n",
    "df_housing = fetch_california_housing()\n",
    "X, y = df_housing.data, df_housing.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create and train a Decision Tree regressor\n",
    "# The following parameters greatly impact the Tree's ability to overfit the data:\n",
    "# - max_depth: default=None\n",
    "# - min_samples_split: default=2\n",
    "# - min_samples_leaf: default=1\n",
    "decision_tree = DecisionTreeRegressor(max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Max. Depth of Trained Tree: {}\".format(decision_tree.tree_.max_depth))\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_pred = decision_tree.predict(X_train)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "print(\"Mean Squared Error (Training): {}\".format(mse))\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (Test): {}\".format(mse))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs Predicted Housing Values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641de912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
