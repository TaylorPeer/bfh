{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eQX8une7lmG"
   },
   "source": [
    "# Machine Learning Workflow: Iris\n",
    "\n",
    "A typical machine learning workflow:\n",
    "\n",
    "1. Dataset Curation\n",
    "2. Dataset Preprocessing\n",
    "3. Dataset Provisioning\n",
    "4. Training Configuration\n",
    "5. Model Training\n",
    "6. Evaluation\n",
    "7. Iterative Optimization\n",
    "\n",
    "We will look at the above workflow based on a commonly used machine learning dataset called the *Iris* Dataset.\n",
    "\n",
    "The Iris Dataset is relatively small and has the following characteristics:\n",
    "\n",
    "* 4 Features (Attributes)\n",
    "* 150 Samples (Instances, Rows)\n",
    "\n",
    "You can read more more about the dataset here: https://archive.ics.uci.edu/ml/datasets/iris/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 1] Dataset Curation\n",
    "\n",
    "Luckily in this case the dataset curation has already taken place - by biologist Ronald Fisher back in 1936.\n",
    "\n",
    "In case the data has not already been assembled - which is often the case in real-world applications - the dataset curation step and all the manual efforts in collecting samples and annotation can potentially take much longer than all other steps combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnF83zWP7lmJ"
   },
   "source": [
    "## [Step 2] Dataset Preprocessing\n",
    "\n",
    "The easiest way to load the Iris dataset is to use the built-in functionality of *scikit-learn*.\n",
    "\n",
    "You can load the Iris dataset with the following commmands:\n",
    "\n",
    "``from sklearn import datasets``\n",
    "\n",
    "``iris = datasets.load_iris()``\n",
    "\n",
    "To familiarize ourselves with the structure of a dataset and to get to know the tooling we need to load our own datasets we will load the dataset ourselves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7GvCv3X7lmK"
   },
   "source": [
    "### Downloading the Data\n",
    "\n",
    "The data for the Iris dataset can be downloaded from.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/\n",
    "    \n",
    "The web directory contains two files that are important for us:\n",
    "\n",
    "* iris.data\n",
    "* iris.names\n",
    "\n",
    "On a Linux or Mac OS machine you can use the following commands to download the files to a local directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T20:58:16.456172Z",
     "start_time": "2023-10-22T20:58:16.192576Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJVM0RgZ7lmK",
    "outputId": "a1949e85-6f58-49fe-b10c-ec2c9fc3745d"
   },
   "outputs": [],
   "source": [
    "# wget is a handy command line utility that allows downloading the specified URL:\n",
    "\n",
    "# - iris.data contains the raw data\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
    "    \n",
    "# - Optional: iris.name contains a description of the dataset\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7p-HZka7lmL"
   },
   "source": [
    "### Inspecting the Iris Dataset Format\n",
    "\n",
    "Using the command line (or a text editor) we can inspect the dataset.\n",
    "\n",
    "The `!` operator will allow you to execute command line commands from a Jupyter cell. \n",
    "This should work on all supported operating systems (Mac OS, Linux, Windows).\n",
    "\n",
    "On a Mac or Linux machine you can make use of the following command line commands:\n",
    "\n",
    "* `head` : Show top n lines of a text file\n",
    "* `tail` : Show last n lines of a text file\n",
    "* `cat`  : Print full content of a text file\n",
    "* `wc -l`: Count number of lines of a text file\n",
    "\n",
    "On a Windows machine the following should work:\n",
    "\n",
    "* `more` : Show content of a text file (might hang in Jupyter)\n",
    "* `type` : Print content of a text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BaJTvzS47lmM",
    "outputId": "c9f4ecc3-7f6a-40fd-a0ea-58a2c8d2fcd9"
   },
   "outputs": [],
   "source": [
    "# head and tail are useful command line utilities on a Linux machine that allow us to see the first n or last \n",
    "# n lines of a text file.\n",
    "\n",
    "# Take your time to inspect both files with the head and tail commands. If you know that a file is not too long you can\n",
    "# also make use of the cat command that prints an entire files contents. For large files this is not advised as it can \n",
    "# easily overpower the JavaScript-based rendering on the browser and cause it to crash.\n",
    "\n",
    "!tail -n 150 iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jh3Xc7ro7lmN"
   },
   "source": [
    "As we can see the Iris dataset is a `CSV` (Comma Separated Values) file.\n",
    "\n",
    "Each row contains 5 values: 4 `double` values and a `string`.\n",
    "The 4 `double` values represent the 4 `features` (in this case measurements of the plant).\n",
    "The `string` value represents the `class` (type) of plant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fabyISRj7lmN"
   },
   "source": [
    "## [Step 3] Provision: Loading the Dataset into a Dataframe\n",
    "\n",
    "Provisioning data means making data available in the expected format of a library or application program.\n",
    "In our case we will be using the **scikit-learn** library and the machine learning algorithms that are available as part of it. \n",
    "\n",
    "### Pandas\n",
    "\n",
    "Pandas is a library for handling of dataframes and the loading from and to other dataformats.\n",
    "\n",
    "We will start of with the following initial command:\n",
    "\n",
    "* `import pandas as pd` : Importing as `pd` is a common convention\n",
    "* `pd.read_csv()` : Read CSV input into a dataframe\n",
    "\n",
    "In order to understand how to use the read_csv() method we can use the `?` operator as shown in the cell below.\n",
    "\n",
    "This will pull up documentation for the method parameters, the return type, and example usage (the latter at the end of the documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OJ9Zlqx7lmO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOnZ0ruD7lmO"
   },
   "source": [
    "### Exercise: Load the Iris.data CSV file with pandas\n",
    "\n",
    "Take a look at the documentation of the read_csv() method and load the `Iris.data` file with pandas.\n",
    "In case you are missing the pandas package use the `!conda install -y` functionality to pull and install the package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJv6GTqY7lmO"
   },
   "outputs": [],
   "source": [
    "# load the Iris.data CSV file into a dataframe called iris_dataframe\n",
    "\n",
    "iris_dataframe = pd.read_csv(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "MAO57EFc7lmO",
    "outputId": "25be8895-be88-4b32-85bf-b2e76656fbb9"
   },
   "outputs": [],
   "source": [
    "# You can use the head() method in order to inspect the loaded dataframe.\n",
    "# Your result should look exactly like shown below.\n",
    "# If your result looks different then please have a look at the documentation of the parameters of the read_csv method \n",
    "# and load the data again. \n",
    "\n",
    "iris_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the columns more descriptive names for future reference\n",
    "\n",
    "iris_dataframe.columns = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y7znJUB7lmP"
   },
   "source": [
    "***Exercise***: Inspect the iris_dataframe with the shape, describe(), and len() attributes and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9EB6BPZ7lmQ"
   },
   "outputs": [],
   "source": [
    "# Inspect the iris_dataframe\n",
    "\n",
    "iris_dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2kIJmQP7lmQ"
   },
   "source": [
    "### Exercise: Create an Input and Response Dataframe\n",
    "\n",
    "If we want to train a supervised machine learning model based on the Iris dataset we have to split our original iris_dataframe into an Input and a Response dataframe.\n",
    "\n",
    "* The `Input` dataframe contains the `features` that are the input for the learning and decision making of the machine learning model.\n",
    "* The `Response` (a.k.a. `Target`) dataframe contains the correct expected values (a.k.a answers) that the system is suppposed to learn.\n",
    "\n",
    "***Iris Setosa Classifier***\n",
    "\n",
    "If we want to train a classifier (a machine learning model that predicts the class/type of Setosa flower based on the 4 measurements) then the content of the `Input` and `Response` dataframes would consist of the following:\n",
    "\n",
    "* `Input`: Each row of the dataframe consists of the 4 measurement values\n",
    "* `Response` : Each row of the dataframe consists of the `class` of the flower \n",
    "\n",
    "To create these two dataframes we can use the functionality provided by pandas:\n",
    "\n",
    "* https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#basics holds the full documentation but is not very easy to read\n",
    "\n",
    "The main functionality we will make use of are:\n",
    "\n",
    "***Selecting columns by label:***\n",
    "\n",
    "We can use `[]` brackets to select a subset of a dataframe. To select a set of columns we can use an array of labels (column headings).\n",
    "\n",
    "E.g. `sample_dataframe[[\"sepal_length\", \"class\"]]` would give us back a dataframe consisting of the columns with the ***names*** `sepal_length` and `class`.\n",
    "\n",
    "Use `sample_dataframe.columns` to get the available columns and their labels listed. \n",
    "\n",
    "\n",
    "***Slicing subsets by row:***\n",
    "\n",
    "This can be achieved by using the `[start:end]` operator.\n",
    "It is a pretty simple way to select rows.\n",
    "\n",
    "* `sample_dataframe[0:5]` : Select row 0,1,2,3,4 of sample_dataframe (***end*** is not inclusive)\n",
    "* `sample_dataframe[start:]` : Select everything from start until the end of the dataframe\n",
    "* `sample_dataframe[:end]` : Select everything from the start until end - 1 index of the dataframe\n",
    "* `sample_dataframe[:]` This would select the full dataframe (all rows)\n",
    "\n",
    "This notation is not limited to dataframes. It works with all lists in Python.  For a list the use of this operator will result in a new copy of the dataframe.\n",
    "\n",
    "Better documentation of the slice operator is provided here: https://stackoverflow.com/questions/509211/understanding-slice-notation?page=1&tab=votes#tab-top\n",
    "\n",
    "Based on the above documentation create the `Input` and `Response` dataframes in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaemCnaP7lmR"
   },
   "outputs": [],
   "source": [
    "# There are different naming conventions people use for these dataframes\n",
    "# input, response or X,Y are common. input, target is also commonly used.\n",
    "iris_dataframe_input = \n",
    "iris_dataframe_response = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upTdzrrd7lmR"
   },
   "source": [
    "***Exercise: Inspect the newly created dataframes***\n",
    "\n",
    "Use the tooling we introduced before in order to inspect your newly created dataframes. \n",
    "\n",
    "* `head()`\n",
    "* `shape`\n",
    "* `len()`\n",
    "* `describe()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtLP3cqB7lmR"
   },
   "outputs": [],
   "source": [
    "# Inspect your dataframes with the above tools in order to get familiar with them\n",
    "# Inspecting the intermediary artifacts in the machine learning workflow is a common and crucial task.\n",
    "# It is easy to imagine how one can be off when sub-setting or slicing through the input data by making a mistake. \n",
    "# These kind of errors are usually disastrous in terms of the outcome of the trained model. The earlier we catch them\n",
    "# the less expensive they are to fix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z46Qso1Z7lmR"
   },
   "source": [
    "## [Step 4] Training Configuration\n",
    "\n",
    "The next step consist of creating the configuration for the training. \n",
    "\n",
    "The main dependencies for choosing a training setup are:\n",
    "\n",
    "* The data used for training (data type, quality, amount)\n",
    "* The task we want to solve (what we want the machine learning system to achieve)\n",
    "\n",
    "Based on these two aspects designing the training set up consists of the following steps:\n",
    "\n",
    "1. Choose training algorithm\n",
    "2. Create initial configuration for training algorithm\n",
    "\n",
    "### Criteria for Choosing A ML Algorithm\n",
    "\n",
    "Some main criteria for choosing a training algorithm are the following:\n",
    "\n",
    "* Task Fit : I.e. can the task we want to solve with ML be solved with the given algorithm\n",
    "* Scalability: How scalable in terms of the shape (columns, rows) of the input data is the algorithm \n",
    "    * The amount of features has a major impact on the scalability of algorithms\n",
    "    * The amount of samples (rows) has a major impact on the execution time of the algorithm\n",
    "* Expected Performance: What is the expected accuracy of the algorithm.\n",
    "* Interpretability: How easy, hard is it to understand what is happening in the algorithm. How hard would it be to 'debug' the behaviour of the algorithm.\n",
    "* Updatable Learning: Can the learned model be updated with more data at a later stage.\n",
    "* Availability: In the pragmatic sense; is a trusty implementation of the algorithm available (also from a license perspective).\n",
    "* Solution requirements: Do we have requirements from the software solution side? E.g. maximum latency, memory limitations, etc ... . \n",
    "\n",
    "As the above list highlights, choosing the 'right' algorithm is a complex tasks with many potential considerations.\n",
    "On the flip side it means that making the right choices has massive potential value. \n",
    "\n",
    "### Choosing an Initial Configuration\n",
    "\n",
    "The choice of an initial training configuration often depends mainly on:\n",
    "\n",
    "* Stats of the training data \n",
    "     * Hyperparameters often allow us to adjust the training to the amount of the training data\n",
    "* Experience or documented well working configurations \n",
    "     * This is often based on identifying `baselines` that worked well on data that we deem similar to our training data.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKGLbE4J7lmS"
   },
   "source": [
    "### Classification Task\n",
    "\n",
    "One machine learning task that fits well to the Iris dataset is `classification`.\n",
    "`Classification` is the task of assigning a `class` (type) to samples based on the input features. \n",
    "For the Iris dataset that translates to using the features (4 measurements) as input for taking the decision (to classify) which type of Iris plant it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIh0Do-l7lmS"
   },
   "source": [
    "### Classification Algorithm\n",
    "\n",
    "Lets say we aim for classifing the type of Iris based on the input.\n",
    "\n",
    "This is the `task` that we want to solve.\n",
    "\n",
    "The choice of `task` informs the choice of our machine learning algorithm.\n",
    "In this case we choose a `classification algorithm`, also called `classifier`.\n",
    "\n",
    "A commonly used classification algorithm is called `logistic regression`.\n",
    "For our initial classification experiments we will make use of this algorithm type.\n",
    "\n",
    "We can make set up this algorithm by the following steps:\n",
    "\n",
    "* Import: `from sklearn.linear_model import LogisticRegression`\n",
    "* Instantiate Model Class: `LogisticRegression`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVQMc-jT7lmT"
   },
   "outputs": [],
   "source": [
    "# Exercise: Set up the logistic regression model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB0Gj_0K7lmT"
   },
   "source": [
    "## [Step 5] Model Training Run\n",
    "\n",
    "Algorithms in scikit-learn can be trained by using the `fit` method. Calling it `fit` is based on the process of `fitting` the model's weights (also called model parameters) during training.\n",
    "`Fitting` means that the weights of the model are adjusted during the training (a.k.a learning) phase based on the input data we have seen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5uPDsOJ7lmT"
   },
   "source": [
    "### Training Run\n",
    "\n",
    "Training a model based on the input data is often referred to as executing or making a `training run`; or simply just a `run`. \n",
    "\n",
    "Common ways to use this terminology are e.g.:\n",
    "\n",
    "* \"I have made a run with the following input data and these hyperparameters\".\n",
    "* Which parameters (meaning hyperparameters) were used for the run?\n",
    "* What was the best run?\n",
    "\n",
    "Running training in scikit-learn can be based on executing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVGfs1En7lmT"
   },
   "outputs": [],
   "source": [
    "trained_model = classifier.fit(iris_dataframe_input.values, iris_dataframe_response.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXYlm8bJ7lmU"
   },
   "source": [
    "### Testing our trained model\n",
    "\n",
    "For a first easy test we can used the `predict` method as shown below.\n",
    "The predict method allows us to pass in data with the same format as the input data.\n",
    "\n",
    "As an exercise try passing in data with a different format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJQTtktp7lmV",
    "outputId": "69bb92b2-7f3c-4dd9-a887-9d02ce622150"
   },
   "outputs": [],
   "source": [
    "trained_model.predict([[0.5,5.0,2.0,1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDp5Qoj77lmV"
   },
   "source": [
    "You can also use the slicing approach we introduced earlier to pass in a subset of the rows of our input data to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUcUHmq67lmV",
    "outputId": "59cb742e-071f-44ff-b96f-e4be8947fcef"
   },
   "outputs": [],
   "source": [
    "trained_model.predict(iris_dataframe_input[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ymg4QJ647lmV"
   },
   "source": [
    "## [Step 6] Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHsfiJ4B7lmV",
    "outputId": "066832d8-5deb-43bc-be8a-889bd67d4d1c"
   },
   "outputs": [],
   "source": [
    "trained_model.score(iris_dataframe_input, iris_dataframe_response.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dov2Fjqn7lmV"
   },
   "source": [
    "## Splitting into Train and Test\n",
    "\n",
    "\n",
    "If we use the same samples for the training of the model and its evaluation, the measured performance is likely a bad\n",
    "indicator of the performance we can expect when we let the model take decisions on data it has not seen before.\n",
    "\n",
    "Testing with the same data that was used for training will usually result in a much higher score. This is due to the effect of fitting the weights the model learns completely on the seen samples.\n",
    "\n",
    "Because of that, the standard approach is to split the available labeled data into two parts before we start training of the ML model:\n",
    "* Train Portion\n",
    "* Test Portion\n",
    "\n",
    "The train portion is the part of the dataset that is used for the training of the model.\n",
    "The test portion is the part of the dataset that is ***excluded*** from the training of the model.\n",
    "This is also called ***holding out*** part of the data. This ***unseen data*** (i.e. data that the model has never seen before) is then used to evaluate how well the model is doing.\n",
    "\n",
    "Typical splits between train and test are 70/30 or 80/20. \n",
    "\n",
    "\n",
    "### Considering the Order of the Input Data\n",
    "\n",
    "When splitting the data into a test and train portion it is important to try to split the data in random fashion. \n",
    "We try to avoid an in-balance for the frequency of classes in either test or train, or an in-balance in terms of 'hard' and 'easy' cases when splitting the data.\n",
    "\n",
    "The best way to achieve this, is to randomly sample (or shuffle) the input data before making the split. \n",
    "\n",
    "Pandas offers us the `sample(frac=double_value)` method as a simple way to do this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7vHZsJC7lmW"
   },
   "source": [
    "### Exercise: Create Train and Test datasets\n",
    "\n",
    "* Create a Train and Test portion of the input data.\n",
    "* You should take the original `iris_dataframe` as input for this\n",
    "* The iris_dataframe is pre-sorted by class. It is a good example of a case where shuffling or sampling from data before the split is a must. So you should start with this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZ0XQuC_7lmW"
   },
   "outputs": [],
   "source": [
    "iris_df_shuffled = \n",
    "\n",
    "iris_df_train = \n",
    "iris_df_test = \n",
    "\n",
    "iris_df_train_input = \n",
    "iris_df_train_response = \n",
    "\n",
    "iris_df_test_input = \n",
    "iris_df_test_response = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdk81hsR7lmW"
   },
   "source": [
    "### Exercise: Train Classifier on Train Dataset\n",
    "\n",
    "Take the train dataset you have created and use it to re-train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EN4zXQQ37lmW"
   },
   "outputs": [],
   "source": [
    "trained_model = classifier.fit(iris_df_train_input, iris_df_train_response.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZYKsTbZ7lmW"
   },
   "source": [
    "### Exercise: Evaluate Classifier with Test Dataset\n",
    "\n",
    "* Use the test dataset you have created and evaluate the classifier trained on the train dataset.\n",
    "* Execute the splitting, training and evaluation multiple times and observe what happens to the score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgULK7wK7lmW",
    "outputId": "e815e16a-0bdc-47cb-db9c-afa9d9b29815"
   },
   "outputs": [],
   "source": [
    "trained_model.score(iris_df_test_input, iris_df_test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Report\n",
    "\n",
    "The `classification_report` function can be used to generate a detailed breakdown of the evaluation scores on class-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(iris_df_test_response, trained_model.predict(iris_df_test_input), target_names=trained_model.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a chart that visualizes the results of a classifier's output on the evaluation set. It shows a breakdown of the actual (true) label vs. the predicted labels for each class. Ideally, the true and predicted labels should overlap but when the classifier makes mistakes, we can use the confusion matrix to learn what kinds of mistakes it is making (e.g. which classes is it confusing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "id": "7VMjUx8S7lmW",
    "outputId": "1dbd7929-6a14-4e42-f514-f7cad5908f03"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(iris_df_test_response, trained_model.predict(iris_df_test_input))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=trained_model.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Optimizations\n",
    "\n",
    "Scikit-learn allows plugging in various implementations of machine learning algorithms. Experimenting with different algorithms is a good way to develop an intuition for which algorithms work well for which scenarios.\n",
    "\n",
    "As an exercise, experiment with the configurations of the classifiers below and observe the impact on the resulting classification scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YsxOm8I17lmX",
    "outputId": "29df8082-bb91-42fd-f275-35adc4f3a33e"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "# - n_neighbors: number of neighbors to consider (default: 5)\n",
    "# - weights: weighting of distance to neighbors: 'uniform' or 'distance' (default: 'uniform')\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Decision Tree:\n",
    "# - max_depth: maximum depth of decision nodes (default: None)\n",
    "decision_tree = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "# Random Forest\n",
    "# - n_estimators: number of individual decision trees used internally by the model (default: 100)\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# Support Vector Machine:\n",
    "support_vector_machine = SVC()\n",
    "\n",
    "# Naive Bayes:\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "# Logistic Regression:\n",
    "# - max_iter: maximum number of iterations (default: 100)\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "classifiers = [\n",
    "    knn,\n",
    "    decision_tree,\n",
    "    random_forest,\n",
    "    support_vector_machine,\n",
    "    naive_bayes,\n",
    "    logistic_regression\n",
    "]\n",
    "\n",
    "model_metrics = []\n",
    "for classifier in tqdm(classifiers):\n",
    "    \n",
    "    # Train the classifier\n",
    "    start_time = time.time()\n",
    "    trained_model = classifier.fit(iris_df_train_input, iris_df_train_response.values.ravel())\n",
    "    end_training_time = time.time()\n",
    "    training_time_elapsed = end_training_time - start_time\n",
    "    \n",
    "    # Apply trained classifier to test set\n",
    "    start_time = time.time()\n",
    "    predictions = trained_model.predict(iris_df_test_input)\n",
    "    prediction_time = time.time()\n",
    "    prediction_time_elapsed = prediction_time - start_time\n",
    "    \n",
    "    # Measure model performance\n",
    "    score = classifier.score(iris_df_test_input, iris_df_test_response)\n",
    "    #print(model_score)\n",
    "    \n",
    "    # Record model metrics\n",
    "    model_metrics.append({\n",
    "        \"model\": trained_model.__class__.__name__,\n",
    "        \"training_time\": training_time_elapsed,\n",
    "        \"prediction_time\": prediction_time_elapsed,\n",
    "        \"score\": score,\n",
    "    })\n",
    "    \n",
    "# Print model metrics table\n",
    "pd.DataFrame(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability\n",
    "\n",
    "Some models are easier to interpret than others. For instance, if we'd like to generate a set of rules for people to be able to identify Iris plants in the wild without using a computer, a simple Decision Tree model would likely be preferable over a neural network, even if the neural network might be some percentage points more accurate.\n",
    "\n",
    "How important the explainability of a model is will often depend on the use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17wBNmXV8Nry"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree.plot_tree(decision_tree, feature_names=iris_dataframe.columns[:-1], class_names=trained_model.classes_, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
