{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18a85b38",
   "metadata": {},
   "source": [
    "# Text Classification with the 20 Newsgroups Dataset\n",
    "\n",
    "The *20 Newsgroups* dataset was created in the 1990s and contains texts extracted from various Usenet groups dedicated to specific topics. Its official homepage can be found here: http://qwone.com/~jason/20Newsgroups/\n",
    "\n",
    "It is a commonly used dataset for benchmarking text classification approaches. Some examples of state-of-the-art benchmark scores on the dataset can be found here: https://paperswithcode.com/dataset/20-newsgroups\n",
    "\n",
    "The labels of the dataset have been computed from the Usenet group where each message was posted. \"Off-topic\" messages therefore may have unintuitive or seemingly \"wrong\" labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8b540",
   "metadata": {},
   "source": [
    "## Download the Dataset\n",
    "\n",
    "Run the following cell to download the CSV file containing the data. Note in this example we're downloading the data the \"Pythonic\" way, rather than using the Terminal command wget as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "data_url = \"https://raw.githubusercontent.com/TaylorPeer/bfh/main/PoML/data/20_newsgroups.csv\"\n",
    "\n",
    "response = requests.get(data_url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    # Save the file to your working directory\n",
    "    with open(\"20_newsgroups.csv\", \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(\"File downloaded successfully.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c9d5f2",
   "metadata": {},
   "source": [
    "## Inspect the Dataset\n",
    "\n",
    "As with the previous notebook, inspect the downloaded file to find out how to properly load it. Take note of its formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ed66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 2 20_newsgroups.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772f4a1",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Use Pandas to load the dataset into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a61ba9",
   "metadata": {},
   "source": [
    "## Inspect the Dataset\n",
    "\n",
    "Make use of the .unique() function of a dataset to view all the unique values contained in a DataFrame column, in this case, the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe2630",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"newsgroup\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae0792",
   "metadata": {},
   "source": [
    "### Label Distribution\n",
    "\n",
    "Using the groupby() and count() methods, we can calculate the number of examples per class label.\n",
    "\n",
    "Being aware of the underlying distribution of our data is important. Some learning algorithms may be affected by skewed data. Also, our interpretations of evaluation scores depends heavily on the distribution of the class labels as well as the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415bc8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"newsgroup\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4232f533",
   "metadata": {},
   "source": [
    "### Example Texts\n",
    "\n",
    "Take a look at some of the raw data to get a feeling for the kinds of messages to be classified. Being familiar with the data can help with recognizing text that may be problematic for our classifiers later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the amount of text displayed when rendering DataFrames\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2966c01",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "In the previous notebook using the Iris dataset, the raw features (petal and sepal widths and lengths) of the flowers to be classifed were already in a format that could be processed by machine learning algorithms so no explicit vectorization step was necessary. This is not the case when working with text data, and we will have to introduce a new explicit vectorization step. Even advanced text processing systems like ChatGPT do not work directly on text internally, but also use vector representations.\n",
    "\n",
    "One way to vectorize text is to regard every word as a feature. In that case the length of our feature vector would equate to the number of unique words contained in our dataset. For each document in the dataset, we would then create a feature vector to represent it, with a 1 for every feature word contained in the document and a 0 for every other word present in the dataset but not contained in that particular document. \n",
    "\n",
    "As you can imagine, the feature vectors would be extremely large and mostly **sparse**, meaning they would mostly contain 0s, since most individual documents will not contain very many of the possible words. The size of these vectors presents a challenge for many learning algorithms and is often referred to as the **curse of dimensionality**.\n",
    "\n",
    "Many **hyperparameters** are available that influence the size of these feature vectors to help keep them to a manageable size.\n",
    "\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "Rather than only using 1s and 0s to represent the presence and absense of feature terms, the concept of scoring the importance of terms has also been introduced. One way to compute scores for terms is via a TF-IDF calculation. TF-IDF combines two simple assumptions to assign scores to terms. The first assumption is that if a term appears frequently in a document, then it must be important in that document, e.g. the document is likely about that topic. The second assumption is that if a term appears frequently in the entire document collection, it is likely not that important, for instance, because it is a commonly used term overall. TF-IDF takes both of these assumptions into consideration and weights terms more highly for a document if they occur often in that document and weights them lower if they occur often overall in the collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c901b2c",
   "metadata": {},
   "source": [
    "### Feature Vectorization of Text Using TF-IDF\n",
    "\n",
    "Run the following cell to vectorize the dataframe and inspect its new representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the 'Text' column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f526f080",
   "metadata": {},
   "source": [
    "### TfidfVectorizer Hyperparameters\n",
    "\n",
    "Use the *min_df* and *max_df* parameters of the TfidfVectorizer to adjust the vectorization of our dataframe. \n",
    "\n",
    "*min_df* represents the minimum amount of documents that must contain a term in order for it to be used as a feature in the vectorization process. This helps us set a sensible lower cutoff to avoid vectorizing typos, spelling mistakes, and other uncommon words that are unlikely to be useful during training.\n",
    "\n",
    "*max_df* represents the maximum amount of documents that can contain a term in order for it to be used as a feature in the vectorization process. This helps us sort out extremely frequent terms that are unlikely to give us any hints about the class label of a document.\n",
    "\n",
    "Note that both parameters accept both floating point values between 0 and 1 to represent a percentage of the document collection as well as an integer value representing an absolute number of documents in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebfc5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=..., min_df=...)\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b31a17",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Before training, let's divide our dataset into training and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"newsgroup\"], test_size=0.2)\n",
    "\n",
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d095f",
   "metadata": {},
   "source": [
    "### Fit the Vectorizer only on the Training Set\n",
    "\n",
    "It's important to fit the vectorizer only on the training set and not the entire dataset - why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc714038",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=20)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a9175b",
   "metadata": {},
   "source": [
    "### Apply the Fitted Vectorizer to the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4699aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorized_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa8384",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17fe757",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = classifier.fit(tfidf_matrix, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96449b32",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = trained_model.predict(tf_idf_vectorized_test)\n",
    "\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    predictions, \n",
    "    target_names=trained_model.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b75135a",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=trained_model.classes_)\n",
    "disp.plot(xticks_rotation=\"vertical\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c03333",
   "metadata": {},
   "source": [
    "### Additional Models\n",
    "\n",
    "Adjust the hyperparameters of the following classifiers and TfidfVectorizer to investigate their impact on the runtime and performance of the models. \n",
    "\n",
    "Some models used in the previous Iris notebook have been excluded here due to their extremely slow performance on large feature vectors (KNeighborsClassifier and SVC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab28458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Decision Tree:\n",
    "# - max_depth: maximum depth of decision nodes (default: None)\n",
    "decision_tree = DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "# Random Forest\n",
    "# - n_estimators: number of individual decision trees used internally by the model (default: 100)\n",
    "random_forest = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "# Naive Bayes:\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "# Logistic Regression:\n",
    "# - max_iter: maximum number of iterations (default: 100)\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "classifiers = [\n",
    "    decision_tree,\n",
    "    random_forest,\n",
    "    naive_bayes,\n",
    "    logistic_regression\n",
    "]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=20)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(X_train)\n",
    "tf_idf_vectorized_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "model_metrics = []\n",
    "for classifier in tqdm(classifiers):\n",
    "    \n",
    "    # Train the classifier\n",
    "    start_time = time.time()\n",
    "    trained_model = classifier.fit(tfidf_matrix, y_train)\n",
    "    end_training_time = time.time()\n",
    "    training_time_elapsed = end_training_time - start_time\n",
    "    \n",
    "    # Apply trained classifier to test set\n",
    "    start_time = time.time()\n",
    "    predictions = trained_model.predict(tf_idf_vectorized_test)\n",
    "    prediction_time = time.time()\n",
    "    prediction_time_elapsed = prediction_time - start_time\n",
    "    \n",
    "    # Measure model performance\n",
    "    score = classifier.score(tf_idf_vectorized_test, y_test)\n",
    "    \n",
    "    # Record model metrics\n",
    "    model_metrics.append({\n",
    "        \"model\": trained_model.__class__.__name__,\n",
    "        \"training_time\": training_time_elapsed,\n",
    "        \"prediction_time\": prediction_time_elapsed,\n",
    "        \"score\": score,\n",
    "    })\n",
    "    \n",
    "# Print model metrics table\n",
    "pd.DataFrame(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e6b23b",
   "metadata": {},
   "source": [
    "### Impact of Training Set Size\n",
    "\n",
    "Since our TfidfVectorizer is \"trained\" on the training set, it only knows words contained in that collection. If this set is very small, our vectorizer will not know many terms and other, new documents to be classified will contain many **out-of-vocabulary** terms. This is an instance where having more data available to train with typically drastically improves model performance - to a point.\n",
    "\n",
    "Run the cell below to view the impact of training on different amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09269908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes:\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "training_set_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "model_metrics = []\n",
    "for training_set_size in tqdm(training_set_sizes):\n",
    "    \n",
    "    X_train_sample = X_train.sample(frac=training_set_size).sort_index()\n",
    "    y_train_sample = y_train[y_train.index.isin(X_train_sample.index)].sort_index()\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=20)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(X_train_sample)\n",
    "    tf_idf_vectorized_test = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    # Train the classifier\n",
    "    start_time = time.time()\n",
    "    trained_model = classifier.fit(tfidf_matrix, y_train_sample)\n",
    "    end_training_time = time.time()\n",
    "    training_time_elapsed = end_training_time - start_time\n",
    "    \n",
    "    # Apply trained classifier to test set\n",
    "    predictions = trained_model.predict(tf_idf_vectorized_test)\n",
    "    \n",
    "    # Measure model performance\n",
    "    score = classifier.score(tf_idf_vectorized_test, y_test)\n",
    "    \n",
    "    # Record model metrics\n",
    "    model_metrics.append({\n",
    "        \"training_set_proportion\": training_set_size,\n",
    "        \"training_set_size\": len(X_train_sample),\n",
    "        \"training_time\": training_time_elapsed,\n",
    "        \"score\": score,\n",
    "    })\n",
    "    \n",
    "# Print model metrics table\n",
    "pd.DataFrame(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f772c5",
   "metadata": {},
   "source": [
    "### Explainability\n",
    "\n",
    "Different models have different internal workings. In the previous Iris notebook we saw how to inspect a Decision Tree classifier to interpret how it classifies our data. Other models have other ways of interpreting their output. \n",
    "\n",
    "Regression models work by learning weights to associate with each feature. These weights are then multiplied with the feature value during prediction time and an internal calculation is used to predict a class label. The value of these weights can be interpreted as a kind of \"importance\" since a higher weight will cause an input feature to indicate in favor of or against a particular class label.\n",
    "\n",
    "The feature weights of our trained Logistic Regression model can be inspected as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6951f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our vectorizer and logistic regression model\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=20)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(X_train)\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "logistic_regression = logistic_regression.fit(tfidf_matrix, y_train)\n",
    "\n",
    "# Get the coefficients from the trained model\n",
    "coefficients = logistic_regression.coef_[0]\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "feature_names = tfidf_df.columns\n",
    "\n",
    "# Create a DataFrame to display the coefficients with feature names\n",
    "coefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "# Sort the DataFrame by the absolute magnitude of coefficients\n",
    "coefficients_df['Abs_Coefficient'] = abs(coefficients_df['Coefficient'])\n",
    "sorted_coefficients_df = coefficients_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame to see the most important features\n",
    "sorted_coefficients_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c643ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
